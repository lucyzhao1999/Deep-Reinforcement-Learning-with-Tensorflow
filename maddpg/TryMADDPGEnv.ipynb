{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# physical/external base state of all entites\n",
    "class EntityState(object):\n",
    "    def __init__(self):\n",
    "        # physical position\n",
    "        self.p_pos = None\n",
    "        # physical velocity\n",
    "        self.p_vel = None\n",
    "\n",
    "# state of agents (including communication and internal/mental state)\n",
    "class AgentState(EntityState):\n",
    "    def __init__(self):\n",
    "        super(AgentState, self).__init__()\n",
    "        # communication utterance\n",
    "        self.c = None\n",
    "\n",
    "# action of the agent\n",
    "class Action(object):\n",
    "    def __init__(self):\n",
    "        # physical action\n",
    "        self.u = None\n",
    "        # communication action\n",
    "        self.c = None\n",
    "\n",
    "# properties and state of physical world entity\n",
    "class Entity(object):\n",
    "    def __init__(self):\n",
    "        # name\n",
    "        self.name = ''\n",
    "        # properties:\n",
    "        self.size = 0.050\n",
    "        # entity can move / be pushed\n",
    "        self.movable = False\n",
    "        # entity collides with others\n",
    "        self.collide = True\n",
    "        # material density (affects mass)\n",
    "        self.density = 25.0\n",
    "        # color\n",
    "        self.color = None\n",
    "        # max speed and accel\n",
    "        self.max_speed = None\n",
    "        self.accel = None\n",
    "        # state\n",
    "        self.state = EntityState()\n",
    "        # mass\n",
    "        self.initial_mass = 1.0\n",
    "\n",
    "    @property\n",
    "    def mass(self):\n",
    "        return self.initial_mass\n",
    "\n",
    "# properties of landmark entities\n",
    "class Landmark(Entity):\n",
    "     def __init__(self):\n",
    "        super(Landmark, self).__init__()\n",
    "\n",
    "# properties of agent entities\n",
    "class Agent(Entity):\n",
    "    def __init__(self):\n",
    "        super(Agent, self).__init__()\n",
    "        # agents are movable by default\n",
    "        self.movable = True\n",
    "        # cannot send communication signals\n",
    "        self.silent = False\n",
    "        # cannot observe the world\n",
    "        self.blind = False\n",
    "        # physical motor noise amount\n",
    "        self.u_noise = None\n",
    "        # communication noise amount\n",
    "        self.c_noise = None\n",
    "        # control range\n",
    "        self.u_range = 1.0\n",
    "        # state\n",
    "        self.state = AgentState()\n",
    "        # action\n",
    "        self.action = Action()\n",
    "        # script behavior to execute\n",
    "        self.action_callback = None\n",
    "\n",
    "# multi-agent world\n",
    "class World(object):\n",
    "    def __init__(self):\n",
    "        # list of agents and entities (can change at execution-time!)\n",
    "        self.agents = []\n",
    "        self.landmarks = []\n",
    "        # communication channel dimensionality\n",
    "        self.dim_c = 0\n",
    "        # position dimensionality\n",
    "        self.dim_p = 2\n",
    "        # color dimensionality\n",
    "        self.dim_color = 3\n",
    "        # simulation timestep\n",
    "        self.dt = 0.1\n",
    "        # physical damping\n",
    "        self.damping = 0.25\n",
    "        # contact response parameters\n",
    "        self.contact_force = 1e+2\n",
    "        self.contact_margin = 1e-3\n",
    "\n",
    "    # return all entities in the world\n",
    "    @property\n",
    "    def entities(self):\n",
    "        return self.agents + self.landmarks\n",
    "\n",
    "    # return all agents controllable by external policies\n",
    "    @property\n",
    "    def policy_agents(self):\n",
    "        return [agent for agent in self.agents if agent.action_callback is None]\n",
    "\n",
    "    # return all agents controlled by world scripts\n",
    "    @property\n",
    "    def scripted_agents(self):\n",
    "        return [agent for agent in self.agents if agent.action_callback is not None]\n",
    "\n",
    "    # update state of the world\n",
    "    def step(self):\n",
    "        # set actions for scripted agents\n",
    "        for agent in self.scripted_agents:\n",
    "            agent.action = agent.action_callback(agent, self)\n",
    "        # gather forces applied to entities\n",
    "        p_force = [None] * len(self.entities)\n",
    "        # apply agent physical controls\n",
    "        p_force = self.apply_action_force(p_force)\n",
    "        # apply environment forces\n",
    "        p_force = self.apply_environment_force(p_force)\n",
    "        # integrate physical state\n",
    "        self.integrate_state(p_force)\n",
    "        # update agent state\n",
    "        for agent in self.agents:\n",
    "            self.update_agent_state(agent)\n",
    "\n",
    "    # gather agent action forces\n",
    "    def apply_action_force(self, p_force):\n",
    "        # set applied forces\n",
    "        for i,agent in enumerate(self.agents):\n",
    "            if agent.movable:\n",
    "                noise = np.random.randn(*agent.action.u.shape) * agent.u_noise if agent.u_noise else 0.0\n",
    "                p_force[i] = agent.action.u + noise\n",
    "        return p_force\n",
    "\n",
    "    # gather physical forces acting on entities\n",
    "    def apply_environment_force(self, p_force):\n",
    "        # simple (but inefficient) collision response\n",
    "        for a,entity_a in enumerate(self.entities):\n",
    "            for b,entity_b in enumerate(self.entities):\n",
    "                if(b <= a): continue\n",
    "                [f_a, f_b] = self.get_collision_force(entity_a, entity_b)\n",
    "                if(f_a is not None):\n",
    "                    if(p_force[a] is None): p_force[a] = 0.0\n",
    "                    p_force[a] = f_a + p_force[a]\n",
    "                if(f_b is not None):\n",
    "                    if(p_force[b] is None): p_force[b] = 0.0\n",
    "                    p_force[b] = f_b + p_force[b]\n",
    "        return p_force\n",
    "\n",
    "    # integrate physical state\n",
    "    def integrate_state(self, p_force):\n",
    "        for i,entity in enumerate(self.entities):\n",
    "            if not entity.movable: continue\n",
    "            entity.state.p_vel = entity.state.p_vel * (1 - self.damping)\n",
    "            if (p_force[i] is not None):\n",
    "                entity.state.p_vel += (p_force[i] / entity.mass) * self.dt\n",
    "            if entity.max_speed is not None:\n",
    "                speed = np.sqrt(np.square(entity.state.p_vel[0]) + np.square(entity.state.p_vel[1]))\n",
    "                if speed > entity.max_speed:\n",
    "                    entity.state.p_vel = entity.state.p_vel / np.sqrt(np.square(entity.state.p_vel[0]) +\n",
    "                                                                  np.square(entity.state.p_vel[1])) * entity.max_speed\n",
    "            entity.state.p_pos += entity.state.p_vel * self.dt\n",
    "\n",
    "    def update_agent_state(self, agent):\n",
    "        # set communication state (directly for now)\n",
    "        if agent.silent:\n",
    "            agent.state.c = np.zeros(self.dim_c)\n",
    "        else:\n",
    "            noise = np.random.randn(*agent.action.c.shape) * agent.c_noise if agent.c_noise else 0.0\n",
    "            agent.state.c = agent.action.c + noise\n",
    "\n",
    "    # get collision forces for any contact between two entities\n",
    "    def get_collision_force(self, entity_a, entity_b):\n",
    "        if (not entity_a.collide) or (not entity_b.collide):\n",
    "            return [None, None] # not a collider\n",
    "        if (entity_a is entity_b):\n",
    "            return [None, None] # don't collide against itself\n",
    "        # compute actual distance between entities\n",
    "        delta_pos = entity_a.state.p_pos - entity_b.state.p_pos\n",
    "        dist = np.sqrt(np.sum(np.square(delta_pos)))\n",
    "        # minimum allowable distance\n",
    "        dist_min = entity_a.size + entity_b.size\n",
    "        # softmax penetration\n",
    "        k = self.contact_margin\n",
    "        penetration = np.logaddexp(0, -(dist - dist_min)/k)*k\n",
    "        force = self.contact_force * delta_pos / dist * penetration\n",
    "        force_a = +force if entity_a.movable else None\n",
    "        force_b = -force if entity_b.movable else None\n",
    "        return [force_a, force_b]\n",
    "\n",
    "\n",
    "class Scenario():\n",
    "    def make_world(self):\n",
    "        world = World()\n",
    "        # add agents\n",
    "        world.agents = [Agent() for i in range(1)]\n",
    "        for i, agent in enumerate(world.agents):\n",
    "            agent.name = 'agent %d' % i\n",
    "            agent.collide = False\n",
    "            agent.silent = True\n",
    "        # add landmarks\n",
    "        world.landmarks = [Landmark() for i in range(1)]\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            landmark.name = 'landmark %d' % i\n",
    "            landmark.collide = False\n",
    "            landmark.movable = False\n",
    "        # make initial conditions\n",
    "        self.reset_world(world)\n",
    "        return world\n",
    "\n",
    "    def reset_world(self, world):\n",
    "        # random properties for agents\n",
    "        for i, agent in enumerate(world.agents):\n",
    "            agent.color = np.array([0.25,0.25,0.25])\n",
    "        # random properties for landmarks\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            landmark.color = np.array([0.75,0.75,0.75])\n",
    "        world.landmarks[0].color = np.array([0.75,0.25,0.25])\n",
    "        # set random initial states\n",
    "        for agent in world.agents:\n",
    "            agent.state.p_pos = np.random.uniform(-1,+1, world.dim_p)\n",
    "            agent.state.p_vel = np.zeros(world.dim_p)\n",
    "            agent.state.c = np.zeros(world.dim_c)\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            landmark.state.p_pos = np.random.uniform(-1,+1, world.dim_p)\n",
    "            landmark.state.p_vel = np.zeros(world.dim_p)\n",
    "\n",
    "    def reward(self, agent, world):\n",
    "        dist2 = np.sum(np.square(agent.state.p_pos - world.landmarks[0].state.p_pos))\n",
    "        return -dist2\n",
    "\n",
    "    def observation(self, agent, world):\n",
    "        # get positions of all entities in this agent's reference frame\n",
    "        entity_pos = []\n",
    "        for entity in world.landmarks:\n",
    "            entity_pos.append(entity.state.p_pos - agent.state.p_pos)\n",
    "        return np.concatenate([agent.state.p_vel] + entity_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentEnv(gym.Env):\n",
    "    metadata = {\n",
    "        'render.modes' : ['human', 'rgb_array']\n",
    "    }\n",
    "\n",
    "    def __init__(self, world, reset_callback=None, reward_callback=None,\n",
    "                 observation_callback=None, info_callback=None,\n",
    "                 done_callback=None, shared_viewer=True):\n",
    "\n",
    "        self.world = world\n",
    "        self.agents = self.world.policy_agents\n",
    "        # set required vectorized gym env property\n",
    "        self.n = len(world.policy_agents)\n",
    "        # scenario callbacks\n",
    "        self.reset_callback = reset_callback\n",
    "        self.reward_callback = reward_callback\n",
    "        self.observation_callback = observation_callback\n",
    "        self.info_callback = info_callback\n",
    "        self.done_callback = done_callback\n",
    "        # environment parameters\n",
    "        self.discrete_action_space = True\n",
    "        # if true, action is a number 0...N, otherwise action is a one-hot N-dimensional vector\n",
    "        self.discrete_action_input = False\n",
    "        # if true, even the action is continuous, action will be performed discretely\n",
    "        self.force_discrete_action = world.discrete_action if hasattr(world, 'discrete_action') else False\n",
    "        # if true, every agent has the same reward\n",
    "        self.shared_reward = world.collaborative if hasattr(world, 'collaborative') else False\n",
    "        self.time = 0\n",
    "\n",
    "        # configure spaces\n",
    "        self.action_space = []\n",
    "        self.observation_space = []\n",
    "        for agent in self.agents:\n",
    "            total_action_space = []\n",
    "            # physical action space\n",
    "            if self.discrete_action_space:\n",
    "                u_action_space = spaces.Discrete(world.dim_p * 2 + 1)\n",
    "            else:\n",
    "                u_action_space = spaces.Box(low=-agent.u_range, high=+agent.u_range, shape=(world.dim_p,), dtype=np.float32)\n",
    "            if agent.movable:\n",
    "                total_action_space.append(u_action_space)\n",
    "            # communication action space\n",
    "            if self.discrete_action_space:\n",
    "                c_action_space = spaces.Discrete(world.dim_c)\n",
    "            else:\n",
    "                c_action_space = spaces.Box(low=0.0, high=1.0, shape=(world.dim_c,), dtype=np.float32)\n",
    "            if not agent.silent:\n",
    "                total_action_space.append(c_action_space)\n",
    "            # total action space\n",
    "            if len(total_action_space) > 1:\n",
    "                # all action spaces are discrete, so simplify to MultiDiscrete action space\n",
    "                if all([isinstance(act_space, spaces.Discrete) for act_space in total_action_space]):\n",
    "                    act_space = MultiDiscrete([[0, act_space.n - 1] for act_space in total_action_space])\n",
    "                else:\n",
    "                    act_space = spaces.Tuple(total_action_space)\n",
    "                self.action_space.append(act_space)\n",
    "            else:\n",
    "                self.action_space.append(total_action_space[0])\n",
    "            # observation space\n",
    "            obs_dim = len(observation_callback(agent, self.world))\n",
    "            self.observation_space.append(spaces.Box(low=-np.inf, high=+np.inf, shape=(obs_dim,), dtype=np.float32))\n",
    "            agent.action.c = np.zeros(self.world.dim_c)\n",
    "\n",
    "        # rendering\n",
    "        self.shared_viewer = shared_viewer\n",
    "        if self.shared_viewer:\n",
    "            self.viewers = [None]\n",
    "        else:\n",
    "            self.viewers = [None] * self.n\n",
    "        self._reset_render()\n",
    "\n",
    "    def step(self, action_n):\n",
    "        obs_n = []\n",
    "        reward_n = []\n",
    "        done_n = []\n",
    "        info_n = {'n': []}\n",
    "        self.agents = self.world.policy_agents\n",
    "        # set action for each agent\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            self._set_action(action_n[i], agent, self.action_space[i])\n",
    "        # advance world state\n",
    "        self.world.step()\n",
    "        # record observation for each agent\n",
    "        for agent in self.agents:\n",
    "            obs_n.append(self._get_obs(agent))\n",
    "            reward_n.append(self._get_reward(agent))\n",
    "            done_n.append(self._get_done(agent))\n",
    "\n",
    "            info_n['n'].append(self._get_info(agent))\n",
    "\n",
    "        # all agents get total reward in cooperative case\n",
    "        reward = np.sum(reward_n)\n",
    "        if self.shared_reward:\n",
    "            reward_n = [reward] * self.n\n",
    "\n",
    "        return obs_n, reward_n, done_n, info_n\n",
    "\n",
    "    def reset(self):\n",
    "        # reset world\n",
    "        self.reset_callback(self.world)\n",
    "        # reset renderer\n",
    "        self._reset_render()\n",
    "        # record observations for each agent\n",
    "        obs_n = []\n",
    "        self.agents = self.world.policy_agents\n",
    "        for agent in self.agents:\n",
    "            obs_n.append(self._get_obs(agent))\n",
    "        return obs_n\n",
    "\n",
    "    # get info used for benchmarking\n",
    "    def _get_info(self, agent):\n",
    "        if self.info_callback is None:\n",
    "            return {}\n",
    "        return self.info_callback(agent, self.world)\n",
    "\n",
    "    # get observation for a particular agent\n",
    "    def _get_obs(self, agent):\n",
    "        if self.observation_callback is None:\n",
    "            return np.zeros(0)\n",
    "        return self.observation_callback(agent, self.world)\n",
    "\n",
    "    # get dones for a particular agent\n",
    "    # unused right now -- agents are allowed to go beyond the viewing screen\n",
    "    def _get_done(self, agent):\n",
    "        if self.done_callback is None:\n",
    "            return False\n",
    "        return self.done_callback(agent, self.world)\n",
    "\n",
    "    # get reward for a particular agent\n",
    "    def _get_reward(self, agent):\n",
    "        if self.reward_callback is None:\n",
    "            return 0.0\n",
    "        return self.reward_callback(agent, self.world)\n",
    "\n",
    "    # set env action for a particular agent\n",
    "    def _set_action(self, action, agent, action_space, time=None):\n",
    "        agent.action.u = np.zeros(self.world.dim_p)\n",
    "        agent.action.c = np.zeros(self.world.dim_c)\n",
    "        # process action\n",
    "        if isinstance(action_space, MultiDiscrete):\n",
    "            act = []\n",
    "            size = action_space.high - action_space.low + 1\n",
    "            index = 0\n",
    "            for s in size:\n",
    "                act.append(action[index:(index+s)])\n",
    "                index += s\n",
    "            action = act\n",
    "        else:\n",
    "            action = [action]\n",
    "\n",
    "        if agent.movable:\n",
    "            # physical action\n",
    "            if self.discrete_action_input:\n",
    "                agent.action.u = np.zeros(self.world.dim_p)\n",
    "                # process discrete action\n",
    "                if action[0] == 1: agent.action.u[0] = -1.0\n",
    "                if action[0] == 2: agent.action.u[0] = +1.0\n",
    "                if action[0] == 3: agent.action.u[1] = -1.0\n",
    "                if action[0] == 4: agent.action.u[1] = +1.0\n",
    "            else:\n",
    "                if self.force_discrete_action:\n",
    "                    d = np.argmax(action[0])\n",
    "                    action[0][:] = 0.0\n",
    "                    action[0][d] = 1.0\n",
    "                if self.discrete_action_space:\n",
    "                    agent.action.u[0] += action[0][1] - action[0][2]\n",
    "                    agent.action.u[1] += action[0][3] - action[0][4]\n",
    "                else:\n",
    "                    agent.action.u = action[0]\n",
    "            sensitivity = 5.0\n",
    "            if agent.accel is not None:\n",
    "                sensitivity = agent.accel\n",
    "            agent.action.u *= sensitivity\n",
    "            action = action[1:]\n",
    "        if not agent.silent:\n",
    "            # communication action\n",
    "            if self.discrete_action_input:\n",
    "                agent.action.c = np.zeros(self.world.dim_c)\n",
    "                agent.action.c[action[0]] = 1.0\n",
    "            else:\n",
    "                agent.action.c = action[0]\n",
    "            action = action[1:]\n",
    "        # make sure we used all elements of action\n",
    "        assert len(action) == 0\n",
    "\n",
    "    # reset rendering assets\n",
    "    def _reset_render(self):\n",
    "        self.render_geoms = None\n",
    "        self.render_geoms_xform = None\n",
    "\n",
    "    # render environment\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "            message = ''\n",
    "            for agent in self.world.agents:\n",
    "                comm = []\n",
    "                for other in self.world.agents:\n",
    "                    if other is agent: continue\n",
    "                    if np.all(other.state.c == 0):\n",
    "                        word = '_'\n",
    "                    else:\n",
    "                        word = alphabet[np.argmax(other.state.c)]\n",
    "                    message += (other.name + ' to ' + agent.name + ': ' + word + '   ')\n",
    "            print(message)\n",
    "\n",
    "        for i in range(len(self.viewers)):\n",
    "            # create viewers (if necessary)\n",
    "            if self.viewers[i] is None:\n",
    "                # import rendering only if we need it (and don't import for headless machines)\n",
    "                #from gym.envs.classic_control import rendering\n",
    "                from multiagent import rendering\n",
    "                self.viewers[i] = rendering.Viewer(700,700)\n",
    "\n",
    "        # create rendering geometry\n",
    "        if self.render_geoms is None:\n",
    "            # import rendering only if we need it (and don't import for headless machines)\n",
    "            #from gym.envs.classic_control import rendering\n",
    "            from multiagent import rendering\n",
    "            self.render_geoms = []\n",
    "            self.render_geoms_xform = []\n",
    "            for entity in self.world.entities:\n",
    "                geom = rendering.make_circle(entity.size)\n",
    "                xform = rendering.Transform()\n",
    "                if 'agent' in entity.name:\n",
    "                    geom.set_color(*entity.color, alpha=0.5)\n",
    "                else:\n",
    "                    geom.set_color(*entity.color)\n",
    "                geom.add_attr(xform)\n",
    "                self.render_geoms.append(geom)\n",
    "                self.render_geoms_xform.append(xform)\n",
    "\n",
    "            # add geoms to viewer\n",
    "            for viewer in self.viewers:\n",
    "                viewer.geoms = []\n",
    "                for geom in self.render_geoms:\n",
    "                    viewer.add_geom(geom)\n",
    "\n",
    "        results = []\n",
    "        for i in range(len(self.viewers)):\n",
    "            from multiagent import rendering\n",
    "            # update bounds to center around agent\n",
    "            cam_range = 1\n",
    "            if self.shared_viewer:\n",
    "                pos = np.zeros(self.world.dim_p)\n",
    "            else:\n",
    "                pos = self.agents[i].state.p_pos\n",
    "            self.viewers[i].set_bounds(pos[0]-cam_range,pos[0]+cam_range,pos[1]-cam_range,pos[1]+cam_range)\n",
    "            # update geometry positions\n",
    "            for e, entity in enumerate(self.world.entities):\n",
    "                self.render_geoms_xform[e].set_translation(*entity.state.p_pos)\n",
    "            # render to display or array\n",
    "            results.append(self.viewers[i].render(return_rgb_array = mode=='rgb_array'))\n",
    "\n",
    "        return results\n",
    "\n",
    "    # create receptor field locations in local coordinate frame\n",
    "    def _make_receptor_locations(self, agent):\n",
    "        receptor_type = 'polar'\n",
    "        range_min = 0.05 * 2.0\n",
    "        range_max = 1.00\n",
    "        dx = []\n",
    "        # circular receptive field\n",
    "        if receptor_type == 'polar':\n",
    "            for angle in np.linspace(-np.pi, +np.pi, 8, endpoint=False):\n",
    "                for distance in np.linspace(range_min, range_max, 3):\n",
    "                    dx.append(distance * np.array([np.cos(angle), np.sin(angle)]))\n",
    "            # add origin\n",
    "            dx.append(np.array([0.0, 0.0]))\n",
    "        # grid receptive field\n",
    "        if receptor_type == 'grid':\n",
    "            for x in np.linspace(-range_max, +range_max, 5):\n",
    "                for y in np.linspace(-range_max, +range_max, 5):\n",
    "                    dx.append(np.array([x,y]))\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scenario():\n",
    "    def make_world(self):\n",
    "        world = World()\n",
    "        # add agents\n",
    "        world.agents = [Agent() for i in range(1)]\n",
    "        for i, agent in enumerate(world.agents):\n",
    "            agent.name = 'agent %d' % i\n",
    "            agent.collide = False\n",
    "            agent.silent = True\n",
    "        # add landmarks\n",
    "        world.landmarks = [Landmark() for i in range(1)]\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            landmark.name = 'landmark %d' % i\n",
    "            landmark.collide = False\n",
    "            landmark.movable = False\n",
    "        # make initial conditions\n",
    "        self.reset_world(world)\n",
    "        return world\n",
    "\n",
    "    def reset_world(self, world):\n",
    "        # random properties for agents\n",
    "        for i, agent in enumerate(world.agents):\n",
    "            agent.color = np.array([0.25,0.25,0.25])\n",
    "        # random properties for landmarks\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            landmark.color = np.array([0.75,0.75,0.75])\n",
    "        world.landmarks[0].color = np.array([0.75,0.25,0.25])\n",
    "        # set random initial states\n",
    "        for agent in world.agents:\n",
    "            agent.state.p_pos = np.random.uniform(-1,+1, world.dim_p)\n",
    "            agent.state.p_vel = np.zeros(world.dim_p)\n",
    "            agent.state.c = np.zeros(world.dim_c)\n",
    "        for i, landmark in enumerate(world.landmarks):\n",
    "            landmark.state.p_pos = np.random.uniform(-1,+1, world.dim_p)\n",
    "            landmark.state.p_vel = np.zeros(world.dim_p)\n",
    "\n",
    "    def reward(self, agent, world):\n",
    "        dist2 = np.sum(np.square(agent.state.p_pos - world.landmarks[0].state.p_pos))\n",
    "        return -dist2\n",
    "\n",
    "    def observation(self, agent, world):\n",
    "        # get positions of all entities in this agent's reference frame\n",
    "        entity_pos = []\n",
    "        for entity in world.landmarks:\n",
    "            entity_pos.append(entity.state.p_pos - agent.state.p_pos)\n",
    "        return np.concatenate([agent.state.p_vel] + entity_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "scenario = Scenario()\n",
    "# create world\n",
    "world = scenario.make_world()\n",
    "# create multiagent environment\n",
    "\n",
    "env = MultiAgentEnv(world, scenario.reset_world, scenario.reward, scenario.observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
